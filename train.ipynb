{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[2024-11-26 15:47:03] \n",
      "Missing Values Analysis:\n",
      "[2024-11-26 15:47:03] \n",
      "Training set missing values:\n",
      "[2024-11-26 15:47:03] age                  0\n",
      "workclass         1836\n",
      "fnlwgt               0\n",
      "education            0\n",
      "education-num        0\n",
      "marital-status       0\n",
      "occupation        1843\n",
      "relationship         0\n",
      "race                 0\n",
      "sex                  0\n",
      "capital-gain         0\n",
      "capital-loss         0\n",
      "hours-per-week       0\n",
      "native-country     583\n",
      "income               0\n",
      "[2024-11-26 15:47:03] \n",
      "Test set missing values:\n",
      "[2024-11-26 15:47:03] age                 0\n",
      "workclass         963\n",
      "fnlwgt              0\n",
      "education           0\n",
      "education-num       0\n",
      "marital-status      0\n",
      "occupation        966\n",
      "relationship        0\n",
      "race                0\n",
      "sex                 0\n",
      "capital-gain        0\n",
      "capital-loss        0\n",
      "hours-per-week      0\n",
      "native-country    274\n",
      "income              0\n",
      "[2024-11-26 15:47:06] \n",
      "Normality Test Results:\n",
      "[2024-11-26 15:47:06] age: p-value = 0.0000\n",
      "[2024-11-26 15:47:06] fnlwgt: p-value = 0.0000\n",
      "[2024-11-26 15:47:06] education-num: p-value = 0.0000\n",
      "[2024-11-26 15:47:06] capital-gain: p-value = 0.0000\n",
      "[2024-11-26 15:47:06] capital-loss: p-value = 0.0000\n",
      "[2024-11-26 15:47:06] hours-per-week: p-value = 0.0000\n",
      "[2024-11-26 15:47:06] \n",
      "Feature Independence Test Results:\n",
      "[2024-11-26 15:47:06] workclass: chi2 = 1045.7086, p-value = 0.0000\n",
      "[2024-11-26 15:47:06] education: chi2 = 4429.6533, p-value = 0.0000\n",
      "[2024-11-26 15:47:06] marital-status: chi2 = 6517.7417, p-value = 0.0000\n",
      "[2024-11-26 15:47:06] occupation: chi2 = 4031.9743, p-value = 0.0000\n",
      "[2024-11-26 15:47:06] relationship: chi2 = 6699.0769, p-value = 0.0000\n",
      "[2024-11-26 15:47:06] race: chi2 = 330.9204, p-value = 0.0000\n",
      "[2024-11-26 15:47:06] sex: chi2 = 1517.8134, p-value = 0.0000\n",
      "[2024-11-26 15:47:06] native-country: chi2 = 317.2304, p-value = 0.0000\n",
      "[2024-11-26 15:47:06] \n",
      "Strong Correlations (|r| > 0.5):\n",
      "[2024-11-26 15:47:06] Training set shape: (32561, 15)\n",
      "[2024-11-26 15:47:06] Test set shape: (16281, 15)\n",
      "[2024-11-26 15:47:07] \n",
      "Feature Engineering:\n",
      "[2024-11-26 15:47:07] - Created financial features: capital_total, has_capital, capital_per_hour\n",
      "[2024-11-26 15:47:07] - Created work-related features: work_intensity\n",
      "[2024-11-26 15:47:07] - Created education level mapping\n",
      "[2024-11-26 15:47:10] \n",
      "Data Preparation:\n",
      "[2024-11-26 15:47:10] Number of features: 14\n",
      "[2024-11-26 15:47:10] Training set balanced shape: (48280, 14)\n",
      "[2024-11-26 15:47:10] Test set shape: (16281, 14)\n",
      "[2024-11-26 15:47:10] \n",
      "Model Training and Evaluation:\n",
      "[2024-11-26 15:47:10] \n",
      "Training random_forest:\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[2024-11-26 15:47:32] Error creating performance plots for random_forest: 'float' object is not iterable\n",
      "[2024-11-26 15:47:32] Cross-validation scores: [0.8529411764705882, 0.8827671913835957, 0.9177713338856669, 0.9149751449875725, 0.9166321458160729]\n",
      "[2024-11-26 15:47:32] Mean CV score: 0.8970 (+/- 0.0513)\n",
      "[2024-11-26 15:47:32] Test set accuracy: 0.8364\n",
      "[2024-11-26 15:47:32] \n",
      "Classification Report:\n",
      "[2024-11-26 15:47:32]               precision    recall  f1-score       support\n",
      "0              0.919687  0.861037  0.889397  12435.000000\n",
      "1              0.627506  0.756890  0.686152   3846.000000\n",
      "accuracy       0.836435  0.836435  0.836435      0.836435\n",
      "macro avg      0.773597  0.808964  0.787774  16281.000000\n",
      "weighted avg   0.850666  0.836435  0.841385  16281.000000\n",
      "[2024-11-26 15:47:32] \n",
      "Training xgboost:\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[2024-11-26 15:47:34] Error creating performance plots for xgboost: 'float' object is not iterable\n",
      "[2024-11-26 15:47:34] Cross-validation scores: [0.8373032311516155, 0.8743786246893124, 0.9012013256006628, 0.9078293289146645, 0.90016570008285]\n",
      "[2024-11-26 15:47:34] Mean CV score: 0.8842 (+/- 0.0521)\n",
      "[2024-11-26 15:47:34] Test set accuracy: 0.8436\n",
      "[2024-11-26 15:47:34] \n",
      "Classification Report:\n",
      "[2024-11-26 15:47:34]               precision    recall  f1-score      support\n",
      "0              0.935902  0.853639  0.892880  12435.00000\n",
      "1              0.631504  0.810972  0.710074   3846.00000\n",
      "accuracy       0.843560  0.843560  0.843560      0.84356\n",
      "macro avg      0.783703  0.832306  0.801477  16281.00000\n",
      "weighted avg   0.863995  0.843560  0.849696  16281.00000\n",
      "[2024-11-26 15:47:34] \n",
      "Training lightgbm:\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[LightGBM] [Info] Number of positive: 24140, number of negative: 24140\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2936\n",
      "[LightGBM] [Info] Number of data points in the train set: 48280, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 19312, number of negative: 19312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2938\n",
      "[LightGBM] [Info] Number of data points in the train set: 38624, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 19312, number of negative: 19312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2933\n",
      "[LightGBM] [Info] Number of data points in the train set: 38624, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 19312, number of negative: 19312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2883\n",
      "[LightGBM] [Info] Number of data points in the train set: 38624, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 19312, number of negative: 19312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2880\n",
      "[LightGBM] [Info] Number of data points in the train set: 38624, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 19312, number of negative: 19312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2882\n",
      "[LightGBM] [Info] Number of data points in the train set: 38624, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2024-11-26 15:47:36] Error creating performance plots for lightgbm: 'float' object is not iterable\n",
      "[2024-11-26 15:47:36] Cross-validation scores: [0.831089478044739, 0.8745857497928748, 0.9093827671913836, 0.9142502071251035, 0.9101077050538525]\n",
      "[2024-11-26 15:47:36] Mean CV score: 0.8879 (+/- 0.0636)\n",
      "[2024-11-26 15:47:36] Test set accuracy: 0.8461\n",
      "[2024-11-26 15:47:36] \n",
      "Classification Report:\n",
      "[2024-11-26 15:47:36]               precision    recall  f1-score       support\n",
      "0              0.931133  0.862244  0.895365  12435.000000\n",
      "1              0.640579  0.793812  0.709011   3846.000000\n",
      "accuracy       0.846078  0.846078  0.846078      0.846078\n",
      "macro avg      0.785856  0.828028  0.802188  16281.000000\n",
      "weighted avg   0.862497  0.846078  0.851343  16281.000000\n",
      "[2024-11-26 15:47:36] \n",
      "Training neural network:\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[2024-11-26 15:59:14] Neural network test accuracy: 0.8038\n",
      "[2024-11-26 15:59:14] Neural network best parameters: {'batch_size': 64, 'epochs': 100, 'hidden_sizes': (128, 64, 32), 'lr': 0.0001}\n",
      "\n",
      "Total execution time: 731.03 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n",
    "                           roc_curve, auc, precision_recall_curve, average_precision_score)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "class TorchNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=(100, 50)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # 建立隱藏層\n",
    "        for h in hidden_sizes:\n",
    "            layers.extend([\n",
    "                torch.nn.Linear(prev_size, h),\n",
    "                torch.nn.BatchNorm1d(h),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_size = h\n",
    "        \n",
    "        # 輸出層\n",
    "        layers.append(torch.nn.Linear(prev_size, 2))\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_size, hidden_sizes=(100, 50), batch_size=32, epochs=100, lr=0.001, device=None):\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.model = TorchNN(input_size, hidden_sizes).to(self.device)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = None\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'input_size': self.input_size,\n",
    "            'hidden_sizes': self.hidden_sizes,\n",
    "            'batch_size': self.batch_size,\n",
    "            'epochs': self.epochs,\n",
    "            'lr': self.lr,\n",
    "            'device': self.device\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        # 重新建立模型\n",
    "        self.model = TorchNN(\n",
    "            input_size=self.input_size,\n",
    "            hidden_sizes=self.hidden_sizes\n",
    "        ).to(self.device)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = None\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        batch_size = self.batch_size\n",
    "        epochs = self.epochs\n",
    "        lr = self.lr\n",
    "        # 將數據轉換為 numpy 數組\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        # 轉換數據為 PyTorch tensors\n",
    "        X = torch.FloatTensor(X).to(self.device)\n",
    "        y = torch.LongTensor(y).to(self.device)\n",
    "        \n",
    "        # 創建數據加載器\n",
    "        dataset = torch.utils.data.TensorDataset(X, y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # 優化器\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        # 訓練循環\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "        return self  # 返回 self\n",
    "                \n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        X = np.array(X)\n",
    "        X = torch.FloatTensor(X).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X)\n",
    "            return torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n",
    "\n",
    "    \n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"Custom encoder for numpy data types\"\"\"\n",
    "    def default(self, obj):\n",
    "        try:\n",
    "            if isinstance(obj, (np.integer, np.floating, np.bool_)):\n",
    "                return obj.item()\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif pd.api.types.is_categorical_dtype(obj):\n",
    "                return str(obj)\n",
    "            elif isinstance(obj, pd.Series):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, pd.DataFrame):\n",
    "                return obj.to_dict()\n",
    "            elif hasattr(obj, '_asdict'):  # For namedtuples\n",
    "                return obj._asdict()\n",
    "            elif isinstance(obj, (pd.Int64Dtype, pd.Float64Dtype)):\n",
    "                return str(obj)\n",
    "            return super().default(obj)\n",
    "        except:\n",
    "            return str(obj)\n",
    "class MLAnalysis:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the ML Analysis class with detailed model configurations\n",
    "        \"\"\"\n",
    "        self.create_directories()\n",
    "        self.start_time = time.time()\n",
    "        self.results_text = []\n",
    "        self.feature_importance = None\n",
    "        \n",
    "        # Define hyperparameters with detailed explanations\n",
    "        self.hyperparameters = {\n",
    "            'random_forest': {\n",
    "                'params': {\n",
    "                    # 'n_estimators': [100, 200, 300],      # Number of trees\n",
    "                    # 'max_depth': [10, 20, None],          # Maximum depth of trees\n",
    "                    # 'min_samples_split': [2, 5, 10],      # Minimum samples required to split\n",
    "                    # 'min_samples_leaf': [1, 2, 4],        # Minimum samples in leaf nodes\n",
    "                    # 'max_features': ['sqrt', 'log2'],     # Feature selection method\n",
    "                    # 'bootstrap': [True, False],           # Bootstrap samples\n",
    "                    # 'class_weight': ['balanced', None]    # Class weight consideration\n",
    "                    'n_estimators': [100,],      # Number of trees\n",
    "                    'max_depth': [10,20],          # Maximum depth of trees\n",
    "                    'class_weight': ['balanced']    # Class weight consideration\n",
    "                },\n",
    "                'param_explanations': {\n",
    "                    'n_estimators': 'Controls the number of trees in the forest. More trees provide better accuracy but increase computation time.',\n",
    "                    'max_depth': 'Maximum depth of each tree. None allows unlimited growth, while specific values prevent overfitting.',\n",
    "                    'min_samples_split': 'Minimum samples required to split a node. Higher values prevent overfitting but might underfit.',\n",
    "                    'min_samples_leaf': 'Minimum samples required in a leaf node. Higher values create more conservative trees.',\n",
    "                    'max_features': 'Method for selecting features for splits. sqrt and log2 are common choices for classification.',\n",
    "                    'bootstrap': 'Whether to use bootstrap samples. False means use whole dataset for each tree.',\n",
    "                    'class_weight': 'Handling class imbalance. balanced adjusts weights inversely proportional to frequencies.'\n",
    "                }\n",
    "            },\n",
    "            'xgboost': {\n",
    "                'params': {\n",
    "                    # 'learning_rate': [0.01, 0.1],         # Learning rate\n",
    "                    # 'max_depth': [3, 5, 7],               # Maximum tree depth\n",
    "                    # 'n_estimators': [100, 200],           # Number of boosting rounds\n",
    "                    # 'subsample': [0.8, 1.0],              # Subsample ratio of training instances\n",
    "                    # 'colsample_bytree': [0.8, 1.0],       # Subsample ratio of columns\n",
    "                    # 'min_child_weight': [1, 3, 5],        # Minimum sum of instance weight in child\n",
    "                    # 'gamma': [0, 0.1, 0.2],               # Minimum loss reduction for split\n",
    "                    # 'reg_alpha': [0, 0.1, 0.5],           # L1 regularization\n",
    "                    # 'reg_lambda': [0.1, 1.0]              # L2 regularization\n",
    "                    'learning_rate': [0.1],         # Learning rate\n",
    "                    'max_depth': [3, 6],               # Maximum tree depth\n",
    "                    'n_estimators': [100],           # Number of boosting rounds\n",
    "                  \n",
    "                },\n",
    "                'param_explanations': {\n",
    "                    'learning_rate': 'Controls the contribution of each tree. Lower values mean more conservative boosting.',\n",
    "                    'max_depth': 'Maximum depth of trees. Deeper trees can model more complex patterns but may overfit.',\n",
    "                    'n_estimators': 'Number of boosting rounds. More rounds might improve performance but may overfit.',\n",
    "                    'subsample': 'Fraction of samples used for training each tree. Helps prevent overfitting.',\n",
    "                    'colsample_bytree': 'Fraction of features used for training each tree. Controls feature selection.',\n",
    "                    'min_child_weight': 'Minimum sum of instance weight in child. Controls tree splitting behavior.',\n",
    "                    'gamma': 'Minimum loss reduction required for split. Higher values make algorithm more conservative.',\n",
    "                    'reg_alpha': 'L1 regularization term. Helps create sparse trees.',\n",
    "                    'reg_lambda': 'L2 regularization term. Helps stabilize the model.'\n",
    "                }\n",
    "            },\n",
    "            'lightgbm': {\n",
    "                'params': {\n",
    "                    # 'learning_rate': [0.01, 0.1],          # Learning rate\n",
    "                    # 'num_leaves': [31, 63, 127],           # Maximum number of leaves\n",
    "                    # 'max_depth': [3, 5, 7],                # Maximum tree depth\n",
    "                    # 'n_estimators': [100, 200],            # Number of boosting iterations\n",
    "                    # 'min_child_samples': [20, 50],         # Minimum samples in leaf\n",
    "                    # 'min_child_weight': [0.001, 0.1],      # Minimum sum of instance weight\n",
    "                    # 'min_split_gain': [0.0, 0.1],          # Minimum gain for split\n",
    "                    # 'subsample': [0.8, 1.0],               # Sample ratio of training instances\n",
    "                    # 'colsample_bytree': [0.8, 1.0],        # Feature selection ratio\n",
    "                    # 'reg_alpha': [0.0, 0.1, 0.5],          # L1 regularization\n",
    "                    # 'reg_lambda': [0.0, 0.1, 0.5],         # L2 regularization\n",
    "                    # 'boosting_type': ['gbdt', 'dart']      # Boosting type\n",
    "                    'learning_rate': [0.1],          # Learning rate\n",
    "                    'max_depth': [3,6],                # Maximum tree depth\n",
    "                    'n_estimators': [100],            # Number of boosting iterations\n",
    "                    \n",
    "\n",
    "                    'min_child_samples': [50],         # 增加最小樣本數要求\n",
    "                    'min_child_weight': [0.01],        # 調整最小權重要求\n",
    "                    'subsample': [0.8],                # 使用子採樣防止過擬合\n",
    "                    'colsample_bytree': [0.8],         # 特徵採樣\n",
    "                    'reg_alpha': [0.1],                # 增加一點 L1 正則化\n",
    "                    'reg_lambda': [0.1],               # 增加一點 L2 正則化\n",
    "                    'min_split_gain': [0.1]            # 設置最小分割增益\n",
    "                },\n",
    "                'param_explanations': {\n",
    "                    'learning_rate': 'Step size shrinkage to prevent overfitting. Lower values need more iterations.',\n",
    "                    'num_leaves': 'Maximum number of leaves in one tree. Controls model complexity.',\n",
    "                    'max_depth': 'Maximum depth of the tree. -1 means no limit.',\n",
    "                    'n_estimators': 'Number of boosting iterations. More iterations might improve performance.',\n",
    "                    'min_child_samples': 'Minimum number of data needed in a leaf. Controls overfitting.',\n",
    "                    'min_child_weight': 'Minimum sum of instance weight in leaf. Similar to min_child_samples.',\n",
    "                    'min_split_gain': 'Minimum gain to make a split. Controls tree growth.',\n",
    "                    'subsample': 'Training instance sampling ratio. Helps prevent overfitting.',\n",
    "                    'colsample_bytree': 'Feature sampling ratio for each tree. Controls feature selection.',\n",
    "                    'reg_alpha': 'L1 regularization. Helps create sparse trees.',\n",
    "                    'reg_lambda': 'L2 regularization. Helps create more conservative trees.',\n",
    "                    'boosting_type': 'Algorithm type. DART often provides better accuracy but might be unstable.'\n",
    "                }\n",
    "            },\n",
    "            'neural_network': {\n",
    "                'params': {\n",
    "                    'hidden_sizes': [(50, 25), (128, 64, 32)],\n",
    "                    'batch_size': [64],\n",
    "                    'epochs': [100],\n",
    "                    'lr': [ 0.0001]\n",
    "                    \n",
    "                    \n",
    "                    # 'hidden_layer_sizes': [(50, 25)],  # Layer architecture\n",
    "                    # 'activation': ['relu', 'tanh'],                        # Activation function\n",
    "                    # 'alpha': [0.0001, 0.001, 0.01],                       # L2 penalty parameter\n",
    "                    # 'learning_rate': ['constant', 'adaptive'],             # Learning rate schedule\n",
    "                    # 'max_iter': [100],                               # Maximum iterations\n",
    "                    # 'early_stopping': [True],                             # Early stopping usage\n",
    "                    # 'validation_fraction': [0.1],                         # Validation set size\n",
    "                    # 'batch_size': [ 64,128,256]                        # Batch size for training\n",
    "                    # 'hidden_layer_sizes': [ (50, 25)],  # Layer architecture\n",
    "                    # 'activation': ['relu'],                        # Activation function\n",
    "                    # 'alpha': [0.0001],                       # L2 penalty parameter\n",
    "                    # 'learning_rate': [ 'adaptive'],             # Learning rate schedule\n",
    "                    # 'max_iter': [200],                               # Maximum iterations\n",
    "                    # 'early_stopping': [True],                             # Early stopping usage\n",
    "                    # 'validation_fraction': [0.1],                         # Validation set size\n",
    "                    # 'batch_size': ['auto']                        # Batch size for training\n",
    "                },\n",
    "                'param_explanations': {\n",
    "                    'hidden_layer_sizes': 'Architecture of hidden layers. More complex architectures can model more complex patterns.',\n",
    "                    'activation': 'Activation function for hidden layers. ReLU is often default, tanh can work better for some cases.',\n",
    "                    'alpha': 'L2 regularization term. Higher values mean stronger regularization.',\n",
    "                    'learning_rate': 'Learning rate schedule for weight updates. Adaptive can be better for complex problems.',\n",
    "                    'max_iter': 'Maximum number of iterations. Should be increased if model doesnt converge.',\n",
    "                    'early_stopping': 'Whether to use early stopping to prevent overfitting.',\n",
    "                    'validation_fraction': 'Fraction of training data to use for validation.',\n",
    "                    'batch_size': 'Size of minibatches for training. Auto lets algorithm decide best size.'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize experiment tracking\n",
    "        self.experiment_results = {\n",
    "            'data_analysis': {},\n",
    "            'feature_engineering': {},\n",
    "            'model_training': {},\n",
    "            'model_evaluation': {},\n",
    "            'statistical_tests': {},\n",
    "            'parameter_analysis': {}  # New section for parameter analysis\n",
    "        }\n",
    "\n",
    "    def create_directories(self):\n",
    "        \"\"\"\n",
    "        Create necessary directories for storing results and artifacts:\n",
    "        - plots: for all visualizations\n",
    "        - results: for numerical results and reports\n",
    "        - models: for saved model states\n",
    "        - stats: for statistical analysis results\n",
    "        \"\"\"\n",
    "        directories = ['plots', 'results', 'models', 'stats']\n",
    "        for d in directories:\n",
    "            if not os.path.exists(d):\n",
    "                os.makedirs(d)\n",
    "\n",
    "    def log(self, message, category=None):\n",
    "        \"\"\"\n",
    "        Log messages and store them in appropriate categories.\n",
    "        Args:\n",
    "            message (str): The message to log\n",
    "            category (str): The category of the log (e.g., 'data_analysis', 'model_training')\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        formatted_message = f\"[{timestamp}] {message}\"\n",
    "        print(formatted_message)\n",
    "        self.results_text.append(formatted_message)\n",
    "        \n",
    "        if category and category in self.experiment_results:\n",
    "            if 'logs' not in self.experiment_results[category]:\n",
    "                self.experiment_results[category]['logs'] = []\n",
    "            self.experiment_results[category]['logs'].append(formatted_message)\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load and perform initial analysis of the dataset:\n",
    "        - Load training and test data\n",
    "        - Analyze data quality\n",
    "        - Perform basic statistical analysis\n",
    "        - Save initial data analysis results\n",
    "        \"\"\"\n",
    "        # Define column names\n",
    "        columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "                'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "        \n",
    "        # Load data\n",
    "        self.train_data = pd.read_csv('dataset/adult.data', names=columns, skipinitialspace=True)\n",
    "        self.test_data = pd.read_csv('dataset/adult.test', names=columns, skipinitialspace=True, skiprows=1)\n",
    "        self.test_data['income'] = self.test_data['income'].str.replace('.', '')\n",
    "\n",
    "        # Perform initial statistical analysis\n",
    "        self.analyze_data_quality()\n",
    "        self.perform_statistical_analysis()\n",
    "        \n",
    "        # Log results\n",
    "        self.log(f\"Training set shape: {self.train_data.shape}\", 'data_analysis')\n",
    "        self.log(f\"Test set shape: {self.test_data.shape}\", 'data_analysis')\n",
    "\n",
    "    def analyze_data_quality(self):\n",
    "        \"\"\"\n",
    "        Comprehensive data quality analysis:\n",
    "        - Missing values analysis\n",
    "        - Data type analysis\n",
    "        - Uniqueness analysis\n",
    "        - Basic statistics\n",
    "        - Save results for reporting\n",
    "        \"\"\"\n",
    "        quality_analysis = {\n",
    "            'missing_values': {\n",
    "                'train': self.train_data.isin(['?']).sum().to_dict(),\n",
    "                'test': self.test_data.isin(['?']).sum().to_dict()\n",
    "            },\n",
    "            'data_types': {\n",
    "                'train': self.train_data.dtypes.to_dict(),\n",
    "                'test': self.test_data.dtypes.to_dict()\n",
    "            },\n",
    "            'unique_values': {\n",
    "                'train': {col: self.train_data[col].nunique() \n",
    "                        for col in self.train_data.columns},\n",
    "                'test': {col: self.test_data[col].nunique() \n",
    "                        for col in self.test_data.columns}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save analysis results\n",
    "        self.experiment_results['data_analysis']['quality_analysis'] = quality_analysis\n",
    "        \n",
    "        # Log key findings\n",
    "        self.log(\"\\nMissing Values Analysis:\", 'data_analysis')\n",
    "        self.log(\"\\nTraining set missing values:\", 'data_analysis')\n",
    "        self.log(pd.Series(quality_analysis['missing_values']['train']).to_string(), 'data_analysis')\n",
    "        self.log(\"\\nTest set missing values:\", 'data_analysis')\n",
    "        self.log(pd.Series(quality_analysis['missing_values']['test']).to_string(), 'data_analysis')\n",
    "        \n",
    "\n",
    "    def save_experiment_state(self):\n",
    "        \"\"\"\n",
    "        Save the current state of the experiment including:\n",
    "        - All numerical results\n",
    "        - Logs\n",
    "        - Configuration\n",
    "        - Timestamps\n",
    "        \"\"\"\n",
    "        # Convert all numpy/pandas types to Python native types\n",
    "        def convert_to_serializable(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_to_serializable(item) for item in obj]\n",
    "            elif isinstance(obj, (np.integer, np.floating, np.bool_)):\n",
    "                return obj.item()\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, pd.Series):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, pd.DataFrame):\n",
    "                return obj.to_dict()\n",
    "            elif pd.api.types.is_categorical_dtype(obj):\n",
    "                return str(obj)\n",
    "            elif isinstance(obj, (pd.Int64Dtype, pd.Float64Dtype)):\n",
    "                return str(obj)\n",
    "            return obj\n",
    "\n",
    "        experiment_data = {\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'duration': time.time() - self.start_time,\n",
    "            'results': convert_to_serializable(self.experiment_results),\n",
    "            'configuration': {\n",
    "                'hyperparameters': convert_to_serializable(self.hyperparameters)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Save as JSON using custom encoder\n",
    "            with open('results/experiment_state.json', 'w') as f:\n",
    "                json.dump(experiment_data, f, indent=4, cls=NumpyEncoder)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save full experiment state due to: {str(e)}\")\n",
    "            # Fallback: Save basic information\n",
    "            basic_data = {\n",
    "                'timestamp': experiment_data['timestamp'],\n",
    "                'duration': experiment_data['duration'],\n",
    "                'configuration': {\n",
    "                    'hyperparameters': self.hyperparameters\n",
    "                }\n",
    "            }\n",
    "            with open('results/experiment_state_basic.json', 'w') as f:\n",
    "                json.dump(basic_data, f, indent=4)\n",
    "            print(\"Saved basic experiment state instead.\")\n",
    "\n",
    "    def create_statistical_plots(self):\n",
    "        \"\"\"\n",
    "        Create and save statistical visualization plots:\n",
    "        - Distribution plots\n",
    "        - Correlation heatmap\n",
    "        - Box plots\n",
    "        - Violin plots\n",
    "        \"\"\"\n",
    "        # 1. Distribution Plots\n",
    "        numerical_features = self.train_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for idx, col in enumerate(numerical_features, 1):\n",
    "            plt.subplot(3, 4, idx)\n",
    "            sns.histplot(data=self.train_data, x=col, hue='income', multiple=\"stack\")\n",
    "            plt.title(f'{col} Distribution by Income')\n",
    "            plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/numerical_distributions.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Correlation Heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        correlation_matrix = self.train_data[numerical_features].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Feature Correlation Matrix')\n",
    "        plt.savefig('plots/correlation_heatmap.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # 3. Box Plots for Numerical Features\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for idx, col in enumerate(numerical_features, 1):\n",
    "            plt.subplot(3, 4, idx)\n",
    "            sns.boxplot(data=self.train_data, x='income', y=col)\n",
    "            plt.title(f'{col} by Income')\n",
    "            plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/boxplots.png')\n",
    "        plt.close()\n",
    "\n",
    "    def log_statistical_findings(self, stats_results):\n",
    "        \"\"\"\n",
    "        Log key statistical findings and insights\n",
    "        \"\"\"\n",
    "        # Log normality test results\n",
    "        self.log(\"\\nNormality Test Results:\", 'statistical_tests')\n",
    "        for feature, result in stats_results['normality_tests'].items():\n",
    "            self.log(f\"{feature}: p-value = {result['p_value']:.4f}\", 'statistical_tests')\n",
    "        \n",
    "        # Log independence test results\n",
    "        self.log(\"\\nFeature Independence Test Results:\", 'statistical_tests')\n",
    "        for feature, result in stats_results['independence_tests'].items():\n",
    "            self.log(f\"{feature}: chi2 = {result['chi2']:.4f}, p-value = {result['p_value']:.4f}\", \n",
    "                    'statistical_tests')\n",
    "        \n",
    "        # Log correlation findings\n",
    "        self.log(\"\\nStrong Correlations (|r| > 0.5):\", 'statistical_tests')\n",
    "        correlation_matrix = pd.DataFrame(stats_results['correlation'])\n",
    "        strong_correlations = []\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                if abs(correlation_matrix.iloc[i,j]) > 0.5:\n",
    "                    strong_correlations.append(\n",
    "                        f\"{correlation_matrix.columns[i]} - {correlation_matrix.columns[j]}: \"\n",
    "                        f\"{correlation_matrix.iloc[i,j]:.4f}\"\n",
    "                    )\n",
    "        for corr in strong_correlations:\n",
    "            self.log(corr, 'statistical_tests')\n",
    "\n",
    "    def feature_engineering(self):\n",
    "        \"\"\"\n",
    "        Comprehensive feature engineering process:\n",
    "        1. Handle missing values\n",
    "        2. Create new features\n",
    "        3. Transform existing features\n",
    "        4. Encode categorical variables\n",
    "        5. Analyze feature importance\n",
    "        \n",
    "        All steps are documented and results are saved\n",
    "        \"\"\"\n",
    "        feature_engineering_results = {}\n",
    "        \n",
    "        # 1. Handle missing values\n",
    "        for data in [self.train_data, self.test_data]:\n",
    "            data.replace('?', np.nan, inplace=True)\n",
    "            for col in ['workclass', 'occupation', 'native-country']:\n",
    "                data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "        \n",
    "        # 2. Create new features\n",
    "        for data in [self.train_data, self.test_data]:\n",
    "            # Financial features\n",
    "            data['capital_total'] = data['capital-gain'] - data['capital-loss']\n",
    "            data['has_capital'] = (data['capital_total'] != 0).astype(int)\n",
    "            data['capital_per_hour'] = data['capital_total'] / (data['hours-per-week'] + 1)\n",
    "            \n",
    "            # Work-related features\n",
    "            data['work_intensity'] = data['hours-per-week'] / data['age']\n",
    "            \n",
    "            # Education mapping\n",
    "            education_map = {\n",
    "                'Preschool': 1, '1st-4th': 1, '5th-6th': 1, '7th-8th': 2, '9th': 2,\n",
    "                '10th': 2, '11th': 2, '12th': 2, 'HS-grad': 3, 'Some-college': 3,\n",
    "                'Assoc-voc': 4, 'Assoc-acdm': 4, 'Bachelors': 5, 'Masters': 6,\n",
    "                'Prof-school': 7, 'Doctorate': 7\n",
    "            }\n",
    "            data['education_level'] = data['education'].map(education_map)\n",
    "        \n",
    "        # Record feature engineering steps\n",
    "        feature_engineering_results['new_features'] = {\n",
    "            'financial_features': ['capital_total', 'has_capital', 'capital_per_hour'],\n",
    "            'work_features': ['work_intensity'],\n",
    "            'education_features': ['education_level']\n",
    "        }\n",
    "        \n",
    "        # Analyze new features\n",
    "        self.analyze_engineered_features()\n",
    "        \n",
    "        # Save results\n",
    "        self.experiment_results['feature_engineering'] = feature_engineering_results\n",
    "        \n",
    "        # Log feature engineering summary\n",
    "        self.log(\"\\nFeature Engineering:\", 'feature_engineering')\n",
    "        self.log(\"- Created financial features: capital_total, has_capital, capital_per_hour\", \n",
    "                'feature_engineering')\n",
    "        self.log(\"- Created work-related features: work_intensity\", 'feature_engineering')\n",
    "        self.log(\"- Created education level mapping\", 'feature_engineering')\n",
    "    def analyze_engineered_features(self):\n",
    "        \"\"\"\n",
    "        Analyze the effectiveness of engineered features:\n",
    "        - Statistical analysis of new features\n",
    "        - Correlation with target variable\n",
    "        - Feature importance analysis\n",
    "        - Visualization of feature distributions\n",
    "        \"\"\"\n",
    "        new_features = ['capital_total', 'has_capital', 'capital_per_hour', \n",
    "                    'work_intensity', 'education_level']\n",
    "        \n",
    "        analysis_results = {}\n",
    "        \n",
    "        # Statistical analysis of new features\n",
    "        for feature in new_features:\n",
    "            analysis_results[feature] = {\n",
    "                'statistics': self.train_data[feature].describe().to_dict(),\n",
    "                'correlation_with_target': stats.pointbiserialr(\n",
    "                    self.train_data[feature],\n",
    "                    (self.train_data['income'] == '>50K').astype(int)\n",
    "                )._asdict()\n",
    "            }\n",
    "        \n",
    "        # Visualize new features\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for idx, feature in enumerate(new_features, 1):\n",
    "            plt.subplot(2, 3, idx)\n",
    "            sns.boxplot(data=self.train_data, x='income', y=feature)\n",
    "            plt.title(f'{feature} by Income')\n",
    "            plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/engineered_features_analysis.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save analysis results\n",
    "        self.experiment_results['feature_engineering']['feature_analysis'] = analysis_results\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepare data for model training:\n",
    "        - Encode categorical variables\n",
    "        - Scale numerical features\n",
    "        - Handle class imbalance\n",
    "        - Split features and target\n",
    "        \"\"\"\n",
    "        # Encode categorical variables\n",
    "        categorical_cols = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                        'relationship', 'race', 'sex', 'native-country']\n",
    "        \n",
    "        self.encoders = {}\n",
    "        for col in categorical_cols:\n",
    "            self.encoders[col] = LabelEncoder()\n",
    "            self.train_data[col] = self.encoders[col].fit_transform(self.train_data[col])\n",
    "            self.test_data[col] = self.encoders[col].transform(self.test_data[col])\n",
    "        \n",
    "        # Select features\n",
    "        self.feature_cols = ['age', 'workclass', 'education-num', 'education_level',\n",
    "                        'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                        'capital_total', 'has_capital', 'work_intensity', \n",
    "                        'capital_per_hour', 'hours-per-week']\n",
    "        \n",
    "        # Prepare X and y\n",
    "        X_train = self.train_data[self.feature_cols]\n",
    "        y_train = (self.train_data['income'] == '>50K').astype(int)\n",
    "        X_test = self.test_data[self.feature_cols]\n",
    "        y_test = (self.test_data['income'] == '>50K').astype(int)\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Handle imbalanced data\n",
    "        smote_tomek = SMOTETomek(random_state=42)\n",
    "        self.X_train_balanced, self.y_train_balanced = smote_tomek.fit_resample(X_train_scaled, y_train)\n",
    "        self.X_test, self.y_test = X_test_scaled, y_test\n",
    "        \n",
    "        # Save preparation results\n",
    "        self.experiment_results['data_preparation'] = {\n",
    "            'feature_columns': self.feature_cols,\n",
    "            'training_shape': self.X_train_balanced.shape,\n",
    "            'test_shape': self.X_test.shape,\n",
    "            'class_distribution': {\n",
    "                'original': np.bincount(y_train).tolist(),\n",
    "                'balanced': np.bincount(self.y_train_balanced).tolist()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.log(\"\\nData Preparation:\", 'data_preparation')\n",
    "        self.log(f\"Number of features: {len(self.feature_cols)}\", 'data_preparation')\n",
    "        self.log(f\"Training set balanced shape: {self.X_train_balanced.shape}\", 'data_preparation')\n",
    "        self.log(f\"Test set shape: {self.X_test.shape}\", 'data_preparation')\n",
    "    \n",
    "    def create_model_comparison_plots(self, model_results):\n",
    "        \"\"\"\n",
    "        Create visualization plots comparing model performance\n",
    "        \"\"\"\n",
    "        # Model accuracy comparison\n",
    "        accuracies = {\n",
    "            name: results['test_performance']['accuracy'] \n",
    "            for name, results in model_results.items()\n",
    "        }\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(accuracies.keys(), accuracies.values())\n",
    "        plt.title('Model Accuracy Comparison')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/model_comparison.png')\n",
    "        plt.close()\n",
    "\n",
    "    def create_model_performance_plots(self, model_name, results, model=None):\n",
    "        \"\"\"\n",
    "        Create detailed performance plots for a single model\n",
    "        Args:\n",
    "            model_name: Name of the model\n",
    "            results: Dictionary containing model results\n",
    "            model: Optional model object (for making predictions)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create feature importance plot if available\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': self.feature_cols,\n",
    "                    'importance': model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
    "                plt.title(f'{model_name} - Feature Importance')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'plots/{model_name.lower()}_feature_importance.png')\n",
    "                plt.close()\n",
    "            \n",
    "            # Create confusion matrix plot\n",
    "            y_true = self.y_test\n",
    "            y_pred = np.array([1 if p > 0.5 else 0 for p in results['test_performance']['classification_report']['1']['precision']])\n",
    "            \n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'{model_name} - Confusion Matrix')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.savefig(f'plots/{model_name.lower()}_confusion_matrix.png')\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log(f\"Error creating performance plots for {model_name}: {str(e)}\", 'model_training')\n",
    "\n",
    "    def train_and_evaluate_model(self, model, name, params=None):\n",
    "        \"\"\"\n",
    "        Train and evaluate a single model with improved settings\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            if params:\n",
    "                grid_search = GridSearchCV(\n",
    "                    model, \n",
    "                    params['params'],\n",
    "                    cv=3,\n",
    "                    scoring='f1',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Fit model\n",
    "                grid_search.fit(self.X_train_balanced, self.y_train_balanced)\n",
    "                \n",
    "                # Store grid search results\n",
    "                results['grid_search_results'] = {\n",
    "                    'params': grid_search.cv_results_['params'],\n",
    "                    'mean_test_score': grid_search.cv_results_['mean_test_score'].tolist(),\n",
    "                    'std_test_score': grid_search.cv_results_['std_test_score'].tolist()\n",
    "                }\n",
    "                \n",
    "                model = grid_search.best_estimator_\n",
    "                results['best_parameters'] = grid_search.best_params_\n",
    "            else:\n",
    "                model.fit(self.X_train_balanced, self.y_train_balanced)\n",
    "            \n",
    "            # Perform cross-validation\n",
    "            cv_scores = cross_val_score(\n",
    "                model, \n",
    "                self.X_train_balanced, \n",
    "                self.y_train_balanced, \n",
    "                cv=5\n",
    "            )\n",
    "            \n",
    "            results['cross_validation'] = {\n",
    "                'scores': cv_scores.tolist(),\n",
    "                'mean': cv_scores.mean(),\n",
    "                'std': cv_scores.std()\n",
    "            }\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            \n",
    "            # y_pred = best_model.predict(self.X_test)\n",
    "            test_accuracy = accuracy_score(self.y_test, y_pred)\n",
    "            results['test_performance'] = {\n",
    "                'accuracy': test_accuracy,\n",
    "                'classification_report': classification_report(\n",
    "                    self.y_test, \n",
    "                    y_pred, \n",
    "                    output_dict=True\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            # Create performance plots with the model object\n",
    "            self.create_model_performance_plots(name, results, model)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log(f\"Error in model training and evaluation: {str(e)}\", 'model_training')\n",
    "            return None\n",
    "    def train_and_evaluate_models(self):\n",
    "        \"\"\"\n",
    "        Train and evaluate multiple models with neural network support\n",
    "        \"\"\"\n",
    "        # 定義基本模型\n",
    "        models = {\n",
    "            'random_forest': (RandomForestClassifier(random_state=42), self.hyperparameters['random_forest']),\n",
    "            'xgboost': (xgb.XGBClassifier(random_state=42), self.hyperparameters['xgboost']),\n",
    "            'lightgbm': (lgb.LGBMClassifier(random_state=42), self.hyperparameters['lightgbm'])\n",
    "        }\n",
    "        \n",
    "        self.log(\"\\nModel Training and Evaluation:\", 'model_training')\n",
    "        model_results = {}\n",
    "        \n",
    "        # 訓練基本模型\n",
    "        for name, (model, params) in models.items():\n",
    "            self.log(f\"\\nTraining {name}:\", 'model_training')\n",
    "            results = self.train_and_evaluate_model(model, name, params)\n",
    "            model_results[name] = results\n",
    "            \n",
    "            # Log results\n",
    "            self.log(f\"Cross-validation scores: {results['cross_validation']['scores']}\", 'model_training')\n",
    "            self.log(f\"Mean CV score: {results['cross_validation']['mean']:.4f} \"\n",
    "                    f\"(+/- {results['cross_validation']['std'] * 2:.4f})\", 'model_training')\n",
    "            self.log(f\"Test set accuracy: {results['test_performance']['accuracy']:.4f}\", 'model_training')\n",
    "            self.log(\"\\nClassification Report:\", 'model_training')\n",
    "            self.log(pd.DataFrame(results['test_performance']['classification_report']).transpose().to_string(), \n",
    "                    'model_training')\n",
    "        \n",
    "        # 訓練神經網路\n",
    "        if 'neural_network' in self.hyperparameters:\n",
    "            try:\n",
    "                self.log(\"\\nTraining neural network:\", 'model_training')\n",
    "                nn_params = self.hyperparameters['neural_network']\n",
    "                \n",
    "                param_grid = {\n",
    "                    'hidden_sizes': nn_params['params']['hidden_sizes'],\n",
    "                    'batch_size': nn_params['params']['batch_size'],\n",
    "                    'epochs': nn_params['params']['epochs'],\n",
    "                    'lr': nn_params['params']['lr']\n",
    "                }\n",
    "                \n",
    "                model = NeuralNetworkClassifier(\n",
    "                    input_size=self.X_train_balanced.shape[1]\n",
    "                )\n",
    "                \n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=3,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=1,  # 避免序列化問題\n",
    "                    verbose=1,\n",
    "                    error_score='raise'\n",
    "                )\n",
    "                \n",
    "                grid_search.fit(self.X_train_balanced, self.y_train_balanced)\n",
    "                \n",
    "                best_model = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                best_score = grid_search.best_score_\n",
    "                \n",
    "                y_pred = best_model.predict(self.X_test)\n",
    "                test_accuracy = accuracy_score(self.y_test, y_pred)\n",
    "                \n",
    "                results = {\n",
    "                    'grid_search_results': {\n",
    "                        'params': grid_search.cv_results_['params'],\n",
    "                        'mean_test_score': grid_search.cv_results_['mean_test_score'].tolist(),\n",
    "                        'std_test_score': grid_search.cv_results_['std_test_score'].tolist()\n",
    "                    },\n",
    "                    'best_parameters': best_params,\n",
    "                    'cross_validation': {\n",
    "                        'mean': best_score,\n",
    "                        'std': grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "                    },\n",
    "                    'test_performance': {\n",
    "                        'accuracy': test_accuracy,\n",
    "                        'classification_report': classification_report(\n",
    "                            self.y_test,\n",
    "                            y_pred,\n",
    "                            output_dict=True\n",
    "                        )\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                model_results['neural_network'] = results\n",
    "                self.log(f\"Neural network test accuracy: {test_accuracy:.4f}\", 'model_training')\n",
    "                self.log(f\"Neural network best parameters: {best_params}\", 'model_training')\n",
    "            except Exception as e:\n",
    "                self.log(f\"Error training neural network: {str(e)}\", 'model_training')\n",
    "\n",
    "        # 保存結果並創建可視化\n",
    "        self.experiment_results['model_training']['model_results'] = model_results\n",
    "        self.create_model_comparison_plots(model_results)\n",
    "\n",
    "    def create_model_comparison_plots(self, model_results):\n",
    "        \"\"\"\n",
    "        Create visualization plots comparing model performance\n",
    "        \"\"\"\n",
    "        # Model accuracy comparison\n",
    "        accuracies = {\n",
    "            name: results['test_performance']['accuracy'] \n",
    "            for name, results in model_results.items()\n",
    "        }\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(accuracies.keys(), accuracies.values())\n",
    "        plt.title('Model Accuracy Comparison')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/model_comparison.png')\n",
    "        plt.close()\n",
    "\n",
    "    def create_model_performance_plots(self, model_name, results, model=None):\n",
    "        \"\"\"\n",
    "        Create detailed performance plots for a single model\n",
    "        Args:\n",
    "            model_name: Name of the model\n",
    "            results: Dictionary containing model results\n",
    "            model: Optional model object (for making predictions)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create feature importance plot if available\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': self.feature_cols,\n",
    "                    'importance': model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
    "                plt.title(f'{model_name} - Feature Importance')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'plots/{model_name.lower()}_feature_importance.png')\n",
    "                plt.close()\n",
    "            \n",
    "            # Create confusion matrix plot\n",
    "            y_true = self.y_test\n",
    "            y_pred = np.array([1 if p > 0.5 else 0 for p in results['test_performance']['classification_report']['1']['precision']])\n",
    "            \n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'{model_name} - Confusion Matrix')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.savefig(f'plots/{model_name.lower()}_confusion_matrix.png')\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log(f\"Error creating performance plots for {model_name}: {str(e)}\", 'model_training')\n",
    "\n",
    "    def perform_statistical_analysis(self):\n",
    "            \"\"\"\n",
    "            Comprehensive statistical analysis of the dataset:\n",
    "            - Descriptive statistics\n",
    "            - Distribution analysis\n",
    "            - Correlation analysis\n",
    "            - Hypothesis testing\n",
    "            - Feature relationships\n",
    "            \"\"\"\n",
    "            stats_results = {}\n",
    "            \n",
    "            # 1. Descriptive Statistics\n",
    "            numerical_features = self.train_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "            categorical_features = self.train_data.select_dtypes(include=['object']).columns\n",
    "            \n",
    "            stats_results['descriptive'] = {\n",
    "                'numerical': {\n",
    "                    'train': self.train_data[numerical_features].describe().to_dict(),\n",
    "                    'test': self.test_data[numerical_features].describe().to_dict()\n",
    "                },\n",
    "                'categorical': {\n",
    "                    'train': {col: self.train_data[col].value_counts().to_dict() \n",
    "                            for col in categorical_features},\n",
    "                    'test': {col: self.test_data[col].value_counts().to_dict() \n",
    "                            for col in categorical_features}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # 2. Normality Tests\n",
    "            stats_results['normality_tests'] = {}\n",
    "            for col in numerical_features:\n",
    "                stat, p_value = stats.normaltest(self.train_data[col].dropna())\n",
    "                stats_results['normality_tests'][col] = {\n",
    "                    'statistic': stat,\n",
    "                    'p_value': p_value\n",
    "                }\n",
    "            \n",
    "            # 3. Feature Independence Tests (Chi-square)\n",
    "            stats_results['independence_tests'] = {}\n",
    "            for col in categorical_features:\n",
    "                if col != 'income':\n",
    "                    contingency_table = pd.crosstab(self.train_data[col], self.train_data['income'])\n",
    "                    chi2, p_value = stats.chi2_contingency(contingency_table)[:2]\n",
    "                    stats_results['independence_tests'][col] = {\n",
    "                        'chi2': chi2,\n",
    "                        'p_value': p_value\n",
    "                    }\n",
    "            \n",
    "            # 4. Correlation Analysis\n",
    "            correlation_matrix = self.train_data[numerical_features].corr()\n",
    "            stats_results['correlation'] = correlation_matrix.to_dict()\n",
    "            \n",
    "            # Save results\n",
    "            self.experiment_results['statistical_tests'] = stats_results\n",
    "            \n",
    "            # Create visualizations\n",
    "            self.create_statistical_plots()\n",
    "            \n",
    "            # Log key findings\n",
    "            self.log_statistical_findings(stats_results)\n",
    "\n",
    "    def analyze_parameter_impact(self, model_name, model_results):\n",
    "        \"\"\"\n",
    "        Analyze the impact of different parameters on model performance\n",
    "        \"\"\"\n",
    "        param_analysis = {\n",
    "            'parameter_importance': {},\n",
    "            'best_params_analysis': {},\n",
    "            'param_performance_correlation': {}\n",
    "        }\n",
    "        \n",
    "        # Get parameter trial results from grid search\n",
    "        if 'best_parameters' in model_results:\n",
    "            grid_results = model_results['grid_search_results']\n",
    "            params_tested = self.hyperparameters[model_name]['params']\n",
    "            \n",
    "            # Analyze each parameter's impact\n",
    "            for param_name in params_tested.keys():\n",
    "                param_values = []\n",
    "                scores = []\n",
    "                for params, score in zip(grid_results['params'], grid_results['mean_test_score']):\n",
    "                    if param_name in params:\n",
    "                        param_values.append(params[param_name])\n",
    "                        scores.append(score)\n",
    "                \n",
    "                # Calculate parameter importance\n",
    "                if len(set(param_values)) > 1:\n",
    "                    correlation = stats.spearmanr(param_values, scores)[0]\n",
    "                    param_analysis['parameter_importance'][param_name] = abs(correlation)\n",
    "                \n",
    "            # Analyze best parameters\n",
    "            best_params = model_results['best_parameters']\n",
    "            param_explanations = self.hyperparameters[model_name.lower()]['param_explanations']\n",
    "            \n",
    "            for param, value in best_params.items():\n",
    "                param_analysis['best_params_analysis'][param] = {\n",
    "                    'value': value,\n",
    "                    'explanation': param_explanations[param],\n",
    "                    'compared_to': f\"Tested values: {params_tested[param]}\"\n",
    "                }\n",
    "        \n",
    "        return param_analysis\n",
    "\n",
    "    def generate_model_report(self, model_name, model_results, param_analysis):\n",
    "        \"\"\"\n",
    "        Generate comprehensive report for model performance and parameter selection\n",
    "        \"\"\"\n",
    "\n",
    "        report = [\n",
    "            f\"\\n{'='*80}\",\n",
    "            f\"\\nDETAILED REPORT FOR {model_name.upper()}\",\n",
    "            f\"{'='*80}\\n\",\n",
    "            \n",
    "            \"\\n1. OVERALL PERFORMANCE\",\n",
    "            f\"{'-'*50}\",\n",
    "        ]\n",
    "\n",
    "        if 'cross_validation' in model_results:\n",
    "            report.extend([\n",
    "                f\"Mean CV Score: {model_results['cross_validation']['mean']:.4f} (±{model_results['cross_validation']['std']*2:.4f})\",\n",
    "            ])\n",
    "        \n",
    "        report.extend([\n",
    "            f\"Test Accuracy: {model_results['test_performance']['accuracy']:.4f}\",\n",
    "            \"\\nClassification Report:\",\n",
    "            pd.DataFrame(model_results['test_performance']['classification_report']).to_string(),\n",
    "            \n",
    "            \"\\n2. PARAMETER ANALYSIS\",\n",
    "            f\"{'-'*50}\",\n",
    "            \"\\nBest Parameters Selected:\",\n",
    "        ])\n",
    "        \n",
    "        # Add parameter analysis\n",
    "        if 'best_params_analysis' in param_analysis:\n",
    "            for param, analysis in param_analysis['best_params_analysis'].items():\n",
    "                report.extend([\n",
    "                    f\"\\n{param}:\",\n",
    "                    f\"  Selected value: {analysis['value']}\",\n",
    "                    f\"  Explanation: {analysis['explanation']}\",\n",
    "                    f\"  Options considered: {analysis['compared_to']}\"\n",
    "                ])\n",
    "        \n",
    "        # Add parameter importance if available\n",
    "        if 'parameter_importance' in param_analysis:\n",
    "            report.extend([\n",
    "                \"\\nParameter Importance Ranking:\",\n",
    "                \"-\" * 30\n",
    "            ])\n",
    "            \n",
    "            # Sort parameters by importance\n",
    "            sorted_params = sorted(\n",
    "                param_analysis['parameter_importance'].items(),\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            for param, importance in sorted_params:\n",
    "                report.append(f\"{param}: {importance:.4f}\")\n",
    "        \n",
    "        # Join all parts of the report\n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "    def save_analysis_reports(self):\n",
    "        \"\"\"\n",
    "        Save comprehensive analysis reports for all models\n",
    "        \"\"\"\n",
    "        # Create main report file\n",
    "        report_path = \"results/model_analysis_report.txt\"\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            # Write overall summary\n",
    "            f.write(\"MACHINE LEARNING MODEL ANALYSIS REPORT\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            # Write model-specific reports\n",
    "            comparison_data = {}\n",
    "            for model_name, results in self.experiment_results['model_training']['model_results'].items():\n",
    "                model_data = {\n",
    "                    'Test Accuracy': results['test_performance']['accuracy']\n",
    "                }\n",
    "                if 'cross_validation' in results:\n",
    "                    model_data['CV Mean Score'] = results['cross_validation']['mean']\n",
    "                    model_data['CV Std'] = results['cross_validation']['std']\n",
    "                comparison_data[model_name] = model_data\n",
    "\n",
    "            comparison_df = pd.DataFrame(comparison_data).T\n",
    "            \n",
    "            # Write comparison summary\n",
    "            f.write(\"\\nMODEL COMPARISON SUMMARY\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "            \n",
    "            f.write(comparison_df.to_string())\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    analysis = MLAnalysis()\n",
    "    \n",
    "    # Execute analysis pipeline\n",
    "    analysis.load_data()\n",
    "    analysis.feature_engineering()\n",
    "    analysis.prepare_data()\n",
    "    analysis.train_and_evaluate_models()\n",
    "    analysis.save_experiment_state()\n",
    "    analysis.save_analysis_reports()\n",
    "    \n",
    "    # Print execution summary\n",
    "    execution_time = time.time() - analysis.start_time\n",
    "    print(f\"\\nTotal execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
