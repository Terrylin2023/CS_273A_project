{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class TorchNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size=1):\n",
    "        super(TorchNN, self).__init__()\n",
    "        layers = []\n",
    "        current_input_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(current_input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            current_input_size = hidden_size\n",
    "        layers.append(nn.Linear(current_input_size, output_size))\n",
    "        layers.append(nn.Sigmoid())  # For binary classification\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_size, hidden_sizes=(64, 32), batch_size=32, epochs=10, lr=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.model = TorchNN(input_size, hidden_sizes)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(X_batch).squeeze()\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.tensor(X, dtype=torch.float32)).squeeze()\n",
    "            return (outputs > 0.5).numpy().astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(torch.tensor(X, dtype=torch.float32)).squeeze()\n",
    "            return outputs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n",
    "                           roc_curve, auc, precision_recall_curve, average_precision_score)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the MLAnalysis class\n",
    "class MLAnalysis:\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.results_text = []\n",
    "        self.feature_importance = None\n",
    "        # self.hyperparameters = {\n",
    "        #     'random_forest': {'n_estimators': [100], 'max_depth': [10, 20], 'class_weight': ['balanced']},\n",
    "        #     'xgboost': {'learning_rate': [0.1], 'max_depth': [3, 6], 'n_estimators': [100]},\n",
    "        #     'lightgbm': {'learning_rate': [0.1], 'max_depth': [3, 6], 'n_estimators': [100]},\n",
    "        # }\n",
    "        self.hyperparameters = {\n",
    "            'random_forest': {\n",
    "                'params': {\n",
    "                    # 'n_estimators': [100, 200, 300],      # Number of trees\n",
    "                    # 'max_depth': [10, 20, None],          # Maximum depth of trees\n",
    "                    # 'min_samples_split': [2, 5, 10],      # Minimum samples required to split\n",
    "                    # 'min_samples_leaf': [1, 2, 4],        # Minimum samples in leaf nodes\n",
    "                    # 'max_features': ['sqrt', 'log2'],     # Feature selection method\n",
    "                    # 'bootstrap': [True, False],           # Bootstrap samples\n",
    "                    # 'class_weight': ['balanced', None]    # Class weight consideration\n",
    "                    'n_estimators': [100,],      # Number of trees\n",
    "                    'max_depth': [10,20],          # Maximum depth of trees\n",
    "                    'class_weight': ['balanced']    # Class weight consideration\n",
    "                },\n",
    "                'param_explanations': {\n",
    "                    'n_estimators': 'Controls the number of trees in the forest. More trees provide better accuracy but increase computation time.',\n",
    "                    'max_depth': 'Maximum depth of each tree. None allows unlimited growth, while specific values prevent overfitting.',\n",
    "                    'min_samples_split': 'Minimum samples required to split a node. Higher values prevent overfitting but might underfit.',\n",
    "                    'min_samples_leaf': 'Minimum samples required in a leaf node. Higher values create more conservative trees.',\n",
    "                    'max_features': 'Method for selecting features for splits. sqrt and log2 are common choices for classification.',\n",
    "                    'bootstrap': 'Whether to use bootstrap samples. False means use whole dataset for each tree.',\n",
    "                    'class_weight': 'Handling class imbalance. balanced adjusts weights inversely proportional to frequencies.'\n",
    "                }\n",
    "            },\n",
    "            'xgboost': {\n",
    "                'params': {\n",
    "                    # 'learning_rate': [0.01, 0.1],         # Learning rate\n",
    "                    # 'max_depth': [3, 5, 7],               # Maximum tree depth\n",
    "                    # 'n_estimators': [100, 200],           # Number of boosting rounds\n",
    "                    # 'subsample': [0.8, 1.0],              # Subsample ratio of training instances\n",
    "                    # 'colsample_bytree': [0.8, 1.0],       # Subsample ratio of columns\n",
    "                    # 'min_child_weight': [1, 3, 5],        # Minimum sum of instance weight in child\n",
    "                    # 'gamma': [0, 0.1, 0.2],               # Minimum loss reduction for split\n",
    "                    # 'reg_alpha': [0, 0.1, 0.5],           # L1 regularization\n",
    "                    # 'reg_lambda': [0.1, 1.0]              # L2 regularization\n",
    "                    'learning_rate': [0.1],         # Learning rate\n",
    "                    'max_depth': [3, 6],               # Maximum tree depth\n",
    "                    'n_estimators': [100],           # Number of boosting rounds\n",
    "                  \n",
    "                },\n",
    "                'param_explanations': {\n",
    "                    'learning_rate': 'Controls the contribution of each tree. Lower values mean more conservative boosting.',\n",
    "                    'max_depth': 'Maximum depth of trees. Deeper trees can model more complex patterns but may overfit.',\n",
    "                    'n_estimators': 'Number of boosting rounds. More rounds might improve performance but may overfit.',\n",
    "                    'subsample': 'Fraction of samples used for training each tree. Helps prevent overfitting.',\n",
    "                    'colsample_bytree': 'Fraction of features used for training each tree. Controls feature selection.',\n",
    "                    'min_child_weight': 'Minimum sum of instance weight in child. Controls tree splitting behavior.',\n",
    "                    'gamma': 'Minimum loss reduction required for split. Higher values make algorithm more conservative.',\n",
    "                    'reg_alpha': 'L1 regularization term. Helps create sparse trees.',\n",
    "                    'reg_lambda': 'L2 regularization term. Helps stabilize the model.'\n",
    "                }\n",
    "            },\n",
    "            'lightgbm': {\n",
    "                'params': {\n",
    "                    # 'learning_rate': [0.01, 0.1],          # Learning rate\n",
    "                    # 'num_leaves': [31, 63, 127],           # Maximum number of leaves\n",
    "                    # 'max_depth': [3, 5, 7],                # Maximum tree depth\n",
    "                    # 'n_estimators': [100, 200],            # Number of boosting iterations\n",
    "                    # 'min_child_samples': [20, 50],         # Minimum samples in leaf\n",
    "                    # 'min_child_weight': [0.001, 0.1],      # Minimum sum of instance weight\n",
    "                    # 'min_split_gain': [0.0, 0.1],          # Minimum gain for split\n",
    "                    # 'subsample': [0.8, 1.0],               # Sample ratio of training instances\n",
    "                    # 'colsample_bytree': [0.8, 1.0],        # Feature selection ratio\n",
    "                    # 'reg_alpha': [0.0, 0.1, 0.5],          # L1 regularization\n",
    "                    # 'reg_lambda': [0.0, 0.1, 0.5],         # L2 regularization\n",
    "                    # 'boosting_type': ['gbdt', 'dart']      # Boosting type\n",
    "                    'learning_rate': [0.1],          # Learning rate\n",
    "                    'max_depth': [3,6],                # Maximum tree depth\n",
    "                    'n_estimators': [100],            # Number of boosting iterations\n",
    "                    \n",
    "\n",
    "                    'min_child_samples': [50],         # 增加最小樣本數要求\n",
    "                    'min_child_weight': [0.01],        # 調整最小權重要求\n",
    "                    'subsample': [0.8],                # 使用子採樣防止過擬合\n",
    "                    'colsample_bytree': [0.8],         # 特徵採樣\n",
    "                    'reg_alpha': [0.1],                # 增加一點 L1 正則化\n",
    "                    'reg_lambda': [0.1],               # 增加一點 L2 正則化\n",
    "                    'min_split_gain': [0.1]            # 設置最小分割增益\n",
    "                },\n",
    "                'param_explanations': {\n",
    "                    'learning_rate': 'Step size shrinkage to prevent overfitting. Lower values need more iterations.',\n",
    "                    'num_leaves': 'Maximum number of leaves in one tree. Controls model complexity.',\n",
    "                    'max_depth': 'Maximum depth of the tree. -1 means no limit.',\n",
    "                    'n_estimators': 'Number of boosting iterations. More iterations might improve performance.',\n",
    "                    'min_child_samples': 'Minimum number of data needed in a leaf. Controls overfitting.',\n",
    "                    'min_child_weight': 'Minimum sum of instance weight in leaf. Similar to min_child_samples.',\n",
    "                    'min_split_gain': 'Minimum gain to make a split. Controls tree growth.',\n",
    "                    'subsample': 'Training instance sampling ratio. Helps prevent overfitting.',\n",
    "                    'colsample_bytree': 'Feature sampling ratio for each tree. Controls feature selection.',\n",
    "                    'reg_alpha': 'L1 regularization. Helps create sparse trees.',\n",
    "                    'reg_lambda': 'L2 regularization. Helps create more conservative trees.',\n",
    "                    'boosting_type': 'Algorithm type. DART often provides better accuracy but might be unstable.'\n",
    "                }\n",
    "            },\n",
    "            'neural_network': {\n",
    "                'params': {\n",
    "                    'hidden_sizes': [(50, 25), (128, 64, 32)],\n",
    "                    'batch_size': [64],\n",
    "                    'epochs': [100],\n",
    "                    'lr': [ 0.0001]\n",
    "                    \n",
    "                    \n",
    "                    # 'hidden_layer_sizes': [(50, 25)],  # Layer architecture\n",
    "                    # 'activation': ['relu', 'tanh'],                        # Activation function\n",
    "                    # 'alpha': [0.0001, 0.001, 0.01],                       # L2 penalty parameter\n",
    "                    # 'learning_rate': ['constant', 'adaptive'],             # Learning rate schedule\n",
    "                    # 'max_iter': [100],                               # Maximum iterations\n",
    "                    # 'early_stopping': [True],                             # Early stopping usage\n",
    "                    # 'validation_fraction': [0.1],                         # Validation set size\n",
    "                    # 'batch_size': [ 64,128,256]                        # Batch size for training\n",
    "                    # 'hidden_layer_sizes': [ (50, 25)],  # Layer architecture\n",
    "                    # 'activation': ['relu'],                        # Activation function\n",
    "                    # 'alpha': [0.0001],                       # L2 penalty parameter\n",
    "                    # 'learning_rate': [ 'adaptive'],             # Learning rate schedule\n",
    "                    # 'max_iter': [200],                               # Maximum iterations\n",
    "                    # 'early_stopping': [True],                             # Early stopping usage\n",
    "                    # 'validation_fraction': [0.1],                         # Validation set size\n",
    "                    # 'batch_size': ['auto']                        # Batch size for training\n",
    "                },\n",
    "                'param_explanations': {\n",
    "                    'hidden_layer_sizes': 'Architecture of hidden layers. More complex architectures can model more complex patterns.',\n",
    "                    'activation': 'Activation function for hidden layers. ReLU is often default, tanh can work better for some cases.',\n",
    "                    'alpha': 'L2 regularization term. Higher values mean stronger regularization.',\n",
    "                    'learning_rate': 'Learning rate schedule for weight updates. Adaptive can be better for complex problems.',\n",
    "                    'max_iter': 'Maximum number of iterations. Should be increased if model doesnt converge.',\n",
    "                    'early_stopping': 'Whether to use early stopping to prevent overfitting.',\n",
    "                    'validation_fraction': 'Fraction of training data to use for validation.',\n",
    "                    'batch_size': 'Size of minibatches for training. Auto lets algorithm decide best size.'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        # self.experiment_results = {}\n",
    "        self.experiment_results = {\n",
    "            'data_analysis': {},\n",
    "            'feature_engineering': {},\n",
    "            'model_training': {},\n",
    "            'model_evaluation': {},\n",
    "            'statistical_tests': {},\n",
    "            'parameter_analysis': {}  # New section for parameter analysis\n",
    "        }\n",
    "\n",
    "    def load_data(self):\n",
    "        columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "                   'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                   'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "        self.train_data = pd.read_csv('dataset/adult.data', names=columns, skipinitialspace=True)\n",
    "        self.test_data = pd.read_csv('dataset/adult.test', names=columns, skipinitialspace=True, skiprows=1)\n",
    "        self.test_data['income'] = self.test_data['income'].str.replace('.', '')\n",
    "        print(f\"Training set shape: {self.train_data.shape}\")\n",
    "        print(f\"Test set shape: {self.test_data.shape}\")\n",
    "\n",
    "    def feature_engineering(self):\n",
    "        for data in [self.train_data, self.test_data]:\n",
    "            data.replace('?', np.nan, inplace=True)\n",
    "            for col in ['workclass', 'occupation', 'native-country']:\n",
    "                data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "            data['capital_total'] = data['capital-gain'] - data['capital-loss']\n",
    "            data['has_capital'] = (data['capital_total'] != 0).astype(int)\n",
    "            data['capital_per_hour'] = data['capital_total'] / (data['hours-per-week'] + 1)\n",
    "            data['work_intensity'] = data['hours-per-week'] / data['age']\n",
    "            education_map = {\n",
    "                'Preschool': 1, '1st-4th': 1, '5th-6th': 1, '7th-8th': 2, '9th': 2,\n",
    "                '10th': 2, '11th': 2, '12th': 2, 'HS-grad': 3, 'Some-college': 3,\n",
    "                'Assoc-voc': 4, 'Assoc-acdm': 4, 'Bachelors': 5, 'Masters': 6,\n",
    "                'Prof-school': 7, 'Doctorate': 7\n",
    "            }\n",
    "            data['education_level'] = data['education'].map(education_map)\n",
    "        print(\"Feature engineering completed.\")\n",
    "\n",
    "    def prepare_data(self):\n",
    "        categorical_cols = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                            'relationship', 'race', 'sex', 'native-country']\n",
    "        self.encoders = {}\n",
    "        for col in categorical_cols:\n",
    "            self.encoders[col] = LabelEncoder()\n",
    "            self.train_data[col] = self.encoders[col].fit_transform(self.train_data[col])\n",
    "            self.test_data[col] = self.encoders[col].transform(self.test_data[col])\n",
    "        \n",
    "        self.feature_cols = ['age', 'workclass', 'education-num', 'education_level',\n",
    "                             'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                             'capital_total', 'has_capital', 'work_intensity', \n",
    "                             'capital_per_hour', 'hours-per-week']\n",
    "        \n",
    "        X_train = self.train_data[self.feature_cols]\n",
    "        y_train = (self.train_data['income'] == '>50K').astype(int)\n",
    "        X_test = self.test_data[self.feature_cols]\n",
    "        y_test = (self.test_data['income'] == '>50K').astype(int)\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        smote_tomek = SMOTETomek(random_state=42)\n",
    "        self.X_train_balanced, self.y_train_balanced = smote_tomek.fit_resample(X_train_scaled, y_train)\n",
    "        self.X_test, self.y_test = X_test_scaled, y_test\n",
    "        print(\"Data preparation completed.\")\n",
    "\n",
    "    def create_model_performance_plots(self, model_name, model, y_pred):\n",
    "        # Ensure the plots directory exists\n",
    "        if not os.path.exists('plots'):\n",
    "            os.makedirs('plots')\n",
    "    \n",
    "        # Create feature importance plot if available\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': self.feature_cols,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
    "            plt.title(f'{model_name} - Feature Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/{model_name.lower()}_feature_importance.png')\n",
    "            plt.close()\n",
    "    \n",
    "        # Create confusion matrix plot\n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'{model_name} - Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/{model_name.lower()}_confusion_matrix.png')\n",
    "        plt.close()\n",
    "    \n",
    "        # Optionally, create other plots such as ROC curves if needed\n",
    "        # Example: ROC curve\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_proba = model.predict_proba(self.X_test)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(self.y_test, y_proba)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc(fpr, tpr):.2f})')\n",
    "            plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "            plt.title(f'{model_name} - ROC Curve')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/{model_name.lower()}_roc_curve.png')\n",
    "            plt.close()\n",
    "    \n",
    "    def train_random_forest(self):\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        params = self.hyperparameters['random_forest']['params']  # Flattened access\n",
    "        grid_search = GridSearchCV(model, param_grid=params, cv=3, scoring='f1', n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(self.X_train_balanced, self.y_train_balanced)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(self.X_test)\n",
    "        \n",
    "        # Record results\n",
    "        self.experiment_results.setdefault('model_training', {}).setdefault('model_results', {})['random_forest'] = {\n",
    "            'test_performance': {\n",
    "                'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "                'classification_report': classification_report(self.y_test, y_pred, output_dict=True)\n",
    "            },\n",
    "            'cross_validation': {\n",
    "                'mean': grid_search.best_score_,\n",
    "                'std': grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "            },\n",
    "            'best_parameters': grid_search.best_params_\n",
    "        }\n",
    "        \n",
    "        # Create performance plots\n",
    "        self.create_model_performance_plots('Random Forest', best_model, y_pred)\n",
    "        \n",
    "        print(\"Random Forest Test Accuracy:\", accuracy_score(self.y_test, y_pred))\n",
    "        print(classification_report(self.y_test, y_pred))\n",
    "    \n",
    "    def train_xgboost(self):\n",
    "        model = xgb.XGBClassifier(random_state=42)\n",
    "        params = self.hyperparameters['xgboost']['params']  # Flattened access\n",
    "        grid_search = GridSearchCV(model, param_grid=params, cv=3, scoring='f1', n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(self.X_train_balanced, self.y_train_balanced)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(self.X_test)\n",
    "        \n",
    "        # Record results\n",
    "        self.experiment_results.setdefault('model_training', {}).setdefault('model_results', {})['xgboost'] = {\n",
    "            'test_performance': {\n",
    "                'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "                'classification_report': classification_report(self.y_test, y_pred, output_dict=True)\n",
    "            },\n",
    "            'cross_validation': {\n",
    "                'mean': grid_search.best_score_,\n",
    "                'std': grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "            },\n",
    "            'best_parameters': grid_search.best_params_\n",
    "        }\n",
    "        \n",
    "        # Create performance plots\n",
    "        self.create_model_performance_plots('XGBoost', best_model, y_pred)\n",
    "        \n",
    "        print(\"XGBoost Test Accuracy:\", accuracy_score(self.y_test, y_pred))\n",
    "        print(classification_report(self.y_test, y_pred))\n",
    "    \n",
    "    def train_lightgbm(self):\n",
    "        model = lgb.LGBMClassifier(random_state=42)\n",
    "        params = self.hyperparameters['lightgbm']['params']  # Flattened access\n",
    "        grid_search = GridSearchCV(model, param_grid=params, cv=3, scoring='f1', n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(self.X_train_balanced, self.y_train_balanced)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(self.X_test)\n",
    "        \n",
    "        # Record results\n",
    "        self.experiment_results.setdefault('model_training', {}).setdefault('model_results', {})['lightgbm'] = {\n",
    "            'test_performance': {\n",
    "                'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "                'classification_report': classification_report(self.y_test, y_pred, output_dict=True)\n",
    "            },\n",
    "            'cross_validation': {\n",
    "                'mean': grid_search.best_score_,\n",
    "                'std': grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "            },\n",
    "            'best_parameters': grid_search.best_params_\n",
    "        }\n",
    "        \n",
    "        # Create performance plots\n",
    "        self.create_model_performance_plots('LightGBM', best_model, y_pred)\n",
    "        \n",
    "        print(\"LightGBM Test Accuracy:\", accuracy_score(self.y_test, y_pred))\n",
    "        print(classification_report(self.y_test, y_pred))\n",
    "\n",
    "    def train_neural_network(self):\n",
    "        # Extract hyperparameters\n",
    "        nn_params = self.hyperparameters['neural_network']['params']\n",
    "        hidden_sizes_list = nn_params['hidden_sizes']\n",
    "        batch_sizes = nn_params['batch_size']\n",
    "        epochs_list = nn_params['epochs']\n",
    "        learning_rates = nn_params['lr']\n",
    "    \n",
    "        best_accuracy = 0\n",
    "        best_model = None\n",
    "        best_params = None\n",
    "    \n",
    "        # Iterate over all combinations of hyperparameters\n",
    "        for hidden_sizes, batch_size, epochs, lr in product(hidden_sizes_list, batch_sizes, epochs_list, learning_rates):\n",
    "            # Initialize the model\n",
    "            model = NeuralNetworkClassifier(input_size=self.X_train_balanced.shape[1],\n",
    "                                            hidden_sizes=hidden_sizes,\n",
    "                                            batch_size=batch_size,\n",
    "                                            epochs=epochs,\n",
    "                                            lr=lr)\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(self.X_train_balanced, self.y_train_balanced)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            accuracy = accuracy_score(self.y_test, y_pred)\n",
    "            \n",
    "            # Check if this is the best model so far\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model\n",
    "                best_params = {\n",
    "                    'hidden_sizes': hidden_sizes,\n",
    "                    'batch_size': batch_size,\n",
    "                    'epochs': epochs,\n",
    "                    'lr': lr\n",
    "                }\n",
    "        \n",
    "        # Record results\n",
    "        y_pred = best_model.predict(self.X_test)\n",
    "        self.experiment_results.setdefault('model_training', {}).setdefault('model_results', {})['neural_network'] = {\n",
    "            'test_performance': {\n",
    "                'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "                'classification_report': classification_report(self.y_test, y_pred, output_dict=True)\n",
    "            },\n",
    "            'best_parameters': best_params\n",
    "        }\n",
    "        \n",
    "        # Create performance plots\n",
    "        self.create_model_performance_plots('Neural Network', best_model, y_pred)\n",
    "        \n",
    "        print(\"Neural Network Test Accuracy:\", accuracy_score(self.y_test, y_pred))\n",
    "        print(classification_report(self.y_test, y_pred))\n",
    "\n",
    "    def save_experiment_state(self):\n",
    "        experiment_data = {\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'duration': time.time() - self.start_time,\n",
    "            'configuration': {'hyperparameters': self.hyperparameters}\n",
    "        }\n",
    "        with open('results/experiment_state.json', 'w') as f:\n",
    "            json.dump(experiment_data, f, indent=4)\n",
    "        print(\"Experiment state saved.\")\n",
    "\n",
    "    def save_analysis_reports(self):\n",
    "        # Ensure the results directory exists\n",
    "        if not os.path.exists('results'):\n",
    "            os.makedirs('results')\n",
    "    \n",
    "        # Create main report file\n",
    "        report_path = \"results/model_analysis_report.txt\"\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            # Write overall summary\n",
    "            f.write(\"MACHINE LEARNING MODEL ANALYSIS REPORT\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            # Write model comparison summary\n",
    "            f.write(\"MODEL COMPARISON SUMMARY\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            \n",
    "            comparison_data = {}\n",
    "            if 'model_training' in self.experiment_results:\n",
    "                for model_name, results in self.experiment_results['model_training']['model_results'].items():\n",
    "                    model_data = {\n",
    "                        'Test Accuracy': results['test_performance']['accuracy'],\n",
    "                        'CV Mean Score': results['cross_validation']['mean'],\n",
    "                        'CV Std': results['cross_validation']['std']\n",
    "                    }\n",
    "                    comparison_data[model_name] = model_data\n",
    "    \n",
    "                # Convert comparison data to DataFrame for better formatting\n",
    "                comparison_df = pd.DataFrame(comparison_data).T\n",
    "                f.write(comparison_df.to_string())\n",
    "                f.write(\"\\n\\n\")\n",
    "    \n",
    "            # Write detailed reports for each model\n",
    "            for model_name, results in self.experiment_results['model_training']['model_results'].items():\n",
    "                f.write(f\"\\n{'='*80}\\n\")\n",
    "                f.write(f\"DETAILED REPORT FOR {model_name.upper()}\\n\")\n",
    "                f.write(f\"{'='*80}\\n\\n\")\n",
    "                \n",
    "                # Overall performance\n",
    "                f.write(\"1. OVERALL PERFORMANCE\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                if 'cross_validation' in results:\n",
    "                    f.write(f\"Mean CV Score: {results['cross_validation']['mean']:.4f} \"\n",
    "                            f\"(±{results['cross_validation']['std']*2:.4f})\\n\")\n",
    "                f.write(f\"Test Accuracy: {results['test_performance']['accuracy']:.4f}\\n\")\n",
    "                f.write(\"\\nClassification Report:\\n\")\n",
    "                f.write(pd.DataFrame(results['test_performance']['classification_report']).to_string())\n",
    "                f.write(\"\\n\\n\")\n",
    "                \n",
    "                # Parameter analysis\n",
    "                if 'best_parameters' in results:\n",
    "                    f.write(\"2. PARAMETER ANALYSIS\\n\")\n",
    "                    f.write(\"-\" * 50 + \"\\n\")\n",
    "                    f.write(\"\\nBest Parameters Selected:\\n\")\n",
    "                    for param, value in results['best_parameters'].items():\n",
    "                        f.write(f\"{param}: {value}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        print(\"Analysis reports saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the analysis class\n",
    "analysis = MLAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (32561, 15)\n",
      "Test set shape: (16281, 15)\n",
      "Feature engineering completed.\n",
      "Data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "analysis.load_data()\n",
    "analysis.feature_engineering()\n",
    "analysis.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    }
   ],
   "source": [
    "analysis.train_random_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "XGBoost Test Accuracy: 0.8485351022664456\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90     12435\n",
      "           1       0.65      0.79      0.71      3846\n",
      "\n",
      "    accuracy                           0.85     16281\n",
      "   macro avg       0.79      0.83      0.80     16281\n",
      "weighted avg       0.86      0.85      0.85     16281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis.train_xgboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "LightGBM Test Accuracy: 0.8457097229899884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.90     12435\n",
      "           1       0.64      0.79      0.71      3846\n",
      "\n",
      "    accuracy                           0.85     16281\n",
      "   macro avg       0.79      0.83      0.80     16281\n",
      "weighted avg       0.86      0.85      0.85     16281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis.train_lightgbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis.train_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment state saved.\n",
      "Analysis reports saved.\n"
     ]
    }
   ],
   "source": [
    "analysis.save_experiment_state()\n",
    "analysis.save_analysis_reports()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
